{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "xiuO6onzIGP7",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "6400759a-6605-489a-f0f7-995e8e722dcf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGYUPlArIlt8"
   },
   "source": [
    "# 1. Import packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y1CiqO5Lep9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bSYncnUtJ7LE",
    "outputId": "c16c8544-ad17-449b-a07a-73c997ab3d46"
   },
   "outputs": [],
   "source": [
    "# !pip -q install demoji\n",
    "!pip -q install torch\n",
    "!pip -q install torch==2.0.1 torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NqJvrqsEIR-T"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minflect\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import demoji\n",
    "import random\n",
    "import inflect\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LDLtFhX9S39p"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qLuTWqXUJ157"
   },
   "outputs": [],
   "source": [
    "file_path = '/content/human_chat.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bN0wWi9MNw_J",
    "outputId": "cc9c56f0-49b8-410b-9606-d5c67dea97a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human 2: What is your favorite holiday?\\n',\n",
       " 'Human 1: one where I get to meet lots of different people.\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OBB3WsDNzy5",
    "outputId": "4567edf1-c6c1-49a0-f942-83a9e209209d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1495"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7Wc4Ss5xN47x"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "\n",
    "    p = inflect.engine()\n",
    "    demoji.download_codes()\n",
    "\n",
    "    text = demoji.replace(text, \"\")\n",
    "\n",
    "    text = re.sub(r'\\b(?:Human 1|Human 2)\\b:?', \" \", text)\n",
    "\n",
    "    text = re.sub(r'\\b\\d+\\b', lambda x: p.number_to_words(x.group()), text)\n",
    "\n",
    "    text = re.sub('[^a-zA-Z\\s]', ' ', text)\n",
    "\n",
    "    text = text.replace(u'\\xa0', u' ').replace('\\u200a', ' ').strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUefzvgSQfT9",
    "outputId": "b60165d9-5abf-40fd-da5f-5f84c20a37e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-85a20fb71f40>:6: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "preprocessed_lines = [preprocess_text(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z6_XL8_YRyQm",
    "outputId": "fd6e7a20-2020-421a-aa60-b13d31a9fa96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'What is your favorite holiday',\n",
       " 'one where I get to meet lots of different people',\n",
       " 'What was the most number of people you have ever met during a holiday',\n",
       " 'Hard to keep a count  Maybe twenty five']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_lines[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "p1SDEVUPR4gR"
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokenizer_conv = [tokenizer(conv) for conv in preprocessed_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBDKbH2dS9mx",
    "outputId": "9b0ac643-a254-4222-8ff9-ea3d2f521434"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'was',\n",
       " 'the',\n",
       " 'most',\n",
       " 'number',\n",
       " 'of',\n",
       " 'people',\n",
       " 'you',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'met',\n",
       " 'during',\n",
       " 'a',\n",
       " 'holiday']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_conv[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "O8IiFji_TA3Z"
   },
   "outputs": [],
   "source": [
    "feature_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    tokenizer_conv,\n",
    "    min_freq = 1,\n",
    "    specials = ['<pad>', '<oov>'],\n",
    "    special_first = True\n",
    ")\n",
    "\n",
    "target_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    tokenizer_conv,\n",
    "    min_freq = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VpW4TJvlx9Q",
    "outputId": "8870390b-5425-4a03-a6cf-9c958d6f5b44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features vocab length: 2749\n",
      "Target vocab length: 2747\n"
     ]
    }
   ],
   "source": [
    "features_vocab_total_words = len(feature_vocab)\n",
    "target_vocab_total_words = len(target_vocab)\n",
    "\n",
    "print(f'Features vocab length: {len(feature_vocab)}')\n",
    "print(f'Target vocab length: {len(target_vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DquE27EKmVNn"
   },
   "outputs": [],
   "source": [
    "def make_ngrams(tokenized_text):\n",
    "    list_ngrams = []\n",
    "    for i in range(1, len(tokenized_text)):\n",
    "        list_ngrams.append(tokenized_text[:i+1])\n",
    "    return list_ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2vCRMsrrtQoD"
   },
   "outputs": [],
   "source": [
    "ngrams_list = []\n",
    "for token in tokenizer_conv:\n",
    "    ngrams_list.extend(make_ngrams(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LMFW2HrtgTz",
    "outputId": "d56cb776-0f57-4deb-eb1a-c92aa8fee364"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what', 'is'],\n",
       " ['what', 'is', 'your'],\n",
       " ['what', 'is', 'your', 'favorite'],\n",
       " ['what', 'is', 'your', 'favorite', 'holiday']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IzwNloCqt_fx"
   },
   "outputs": [],
   "source": [
    "def add_random_oov_tokens(ngram):\n",
    "    for idx, word in enumerate(ngram[:-1]):\n",
    "        if random.uniform(0,1)<0.1:\n",
    "            ngram[idx] = '<oov>'\n",
    "        return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1gSKSWwtkRH",
    "outputId": "cc466b8d-388c-48b4-dbb3-cf41471c3ba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "ngrams_list_oov = []\n",
    "for ngrams in ngrams_list:\n",
    "    ngrams_list_oov.append(add_random_oov_tokens(ngrams))\n",
    "print(any('<oov>' in ngram for ngram in ngrams_list_oov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "voFZgAWruutL"
   },
   "outputs": [],
   "source": [
    "def text_to_numerical_seq(tokenized_text):\n",
    "    tokens_list = []\n",
    "    if tokenized_text[-1] in target_vocab.get_itos():\n",
    "        for token in tokenized_text[:-1]:\n",
    "            num_token = feature_vocab[token] if token in feature_vocab.get_itos() else feature_vocab['<oov>']\n",
    "            tokens_list.append(num_token)\n",
    "        num_token = target_vocab[tokenized_text[-1]]\n",
    "        tokens_list.append(num_token)\n",
    "        return tokens_list\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gQSE59Snweup"
   },
   "outputs": [],
   "source": [
    "\n",
    "input_seq = [text_to_numerical_seq(sequence) for sequence in ngrams_list_oov if text_to_numerical_seq(sequence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-VWo-oWw7Uj",
    "outputId": "3e4d754d-b041-475d-c9c1-f3c6db05257b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input sequences: 18562\n"
     ]
    }
   ],
   "source": [
    "print(f'Total input sequences: {len(input_seq)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e27vQ2Hx3Di",
    "outputId": "b36f31a8-3277-49f2-8aa7-f727875a9ab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56, 90, 2, 73, 2], [56, 90, 2, 73, 4, 228]]\n"
     ]
    }
   ],
   "source": [
    "print(input_seq[7:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5IeQwvKlx_uz",
    "outputId": "7736d0a2-a93b-41a6-94cf-0b021d0177ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [seq[:-1] for seq in input_seq]\n",
    "y = [seq[-1] for seq in input_seq]\n",
    "len(x[0]), y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhF_BWFTy2Wr",
    "outputId": "c1dd8f28-3cbe-4020-c39c-9b5faf89ccc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12] 11\n"
     ]
    }
   ],
   "source": [
    "print(x[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQMGpMI2y4zS",
    "outputId": "674e0d98-ffb0-4105-934c-f5b5353ae187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    }
   ],
   "source": [
    "longest_seq_feature = max(len(seq) for seq in x)\n",
    "print(longest_seq_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4-1Ek37zad5",
    "outputId": "691d3228-8754-4fd9-b9a3-54c00029c977"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  1, 13]),\n",
       " [1, 13, 29],\n",
       " 153)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_x = [F.pad(torch.tensor(seq), (longest_seq_feature - len(seq), 0), value = 0) for seq in x]\n",
    "padded_x[1], x[2], len(padded_x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "AzWFLC-Og4lG"
   },
   "outputs": [],
   "source": [
    "padded_x = torch.stack(padded_x)\n",
    "y = torch.tensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yTR2FyPhE_2",
    "outputId": "e69d9492-ecf1-4402-85de-f557b2e0aa1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y), type(padded_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "4IbPiGmmhKbg"
   },
   "outputs": [],
   "source": [
    "y_one_hot = F.one_hot(y, num_classes = len(target_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "m4BR9n8EhYOe"
   },
   "outputs": [],
   "source": [
    "data =  TensorDataset(padded_x, y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "w-oAS0pRkngB"
   },
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "BfLNhLC6lDL9"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = random_split(data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "K0wmuMyWlM8l"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle= True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "UlqG7EjlmKZ9"
   },
   "outputs": [],
   "source": [
    "class My_BiLSTM(nn.Module):\n",
    "    def __init__(self, features_vocab_total_words, target_vocab_total_words, embedding_dim, hidden_dim):\n",
    "        super(My_BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(features_vocab_total_words, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, target_vocab_total_words)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.embedding.weight.device)\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # Since the LSTM is bidirectional, we concatenate the last hidden state of the forward direction\n",
    "        # and the first hidden state of the backward direction before passing it to the fully connected layer\n",
    "        # For batch_first=True, the last timestep of the forward direction is lstm_out[:, -1, :hidden_dim]\n",
    "        # and the first timestep of the backward direction is lstm_out[:, 0, hidden_dim:]\n",
    "        output = self.fc(torch.cat((lstm_out[:, -1, :hidden_dim], lstm_out[:, 0, hidden_dim:]), dim=1))\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "hEzTfRdKl14f"
   },
   "outputs": [],
   "source": [
    "features_vocab_total_words = 2749\n",
    "target_vocab_total_words = 2747\n",
    "embedding_dim = 128\n",
    "hidden_dim = 200\n",
    "model = My_BiLSTM(features_vocab_total_words, target_vocab_total_words, embedding_dim = embedding_dim, hidden_dim = hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "RS_w3r-hUxhv"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "vEjZBXDiTpmT"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqhEXf2dUQzB",
    "outputId": "b390df4d-5110-4617-e0a3-1fa3e68d0ed2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GB0NH_7zNM7n",
    "outputId": "44960fcd-5bea-4d28-d2d6-b43b29cb9fdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My_BiLSTM(\n",
       "  (embedding): Embedding(2749, 128)\n",
       "  (lstm): LSTM(128, 200, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=400, out_features=2747, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "b6Ghw_CgNUD2"
   },
   "outputs": [],
   "source": [
    "def calculate_topk_accuracy(model, data_loader, k=3):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            output = model(batch_x)\n",
    "\n",
    "            _, predicted_indices = output.topk(k, dim = 1)\n",
    "\n",
    "            correct_predictions += torch.any(predicted_indices == torch.argmax(batch_y, dim =1, keepdim = True), dim = 1).sum().item()\n",
    "            total_predictions += batch_y.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FJr78C3RPLhr"
   },
   "outputs": [],
   "source": [
    "all_accuracies = []\n",
    "all_loses = []\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y.argmax(dim = 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epochs % 2 == 0:\n",
    "        accuracy = calculate_topk_accuracy(model, train_loader)\n",
    "        print(f\"Epoch {epoch}/ {epochs}, Loss: {loss.item():.4f}, Train K-accuracy: {accuracy * 100:.2f}%\")\n",
    "        all_accuracies.append(accuracy)\n",
    "        all_loses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWuwFJdRRW45"
   },
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nm-V5FzQ2IeK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
